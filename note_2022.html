<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Computer Vision Notes</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background: #f9f9f9;
      padding: 40px;
      max-width: 900px;
      margin: auto;
      color: #333;
    }
    h1 {
      text-align: center;
      color: #222;
    }
    h2 {
      color: #444;
      margin-top: 40px;
    }
    pre {
      background: #f0f0f0;
      padding: 10px;
      overflow-x: auto;
      border-left: 3px solid #ccc;
    }
    code {
      font-family: Consolas, monospace;
      font-size: 14px;
    }
  </style>
</head>
<body>
  <h1>Computer Vision Code Notes</h1>

  <h2>Object Tracking with DeepSORT</h2>
  <pre><code>import cv2
import numpy as np
import sys
import glob
import time
import torch

class YoloDetector:
    def __init__(self, model_name):
        self.model = self.load_model(model_name)
        self.classes = self.model.names
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print("Using Device:", self.device)

    def load_model(self, model_name):
        if model_name:
            model = torch.hub.load("ultralytics/yolov5", "custom", path=model_name, force_reload=True)
        else:
            model = torch.hub.load("ultralytics/yolov5", "yolov5s", pretrained=True)
        return model

    def score_frame(self, frame):
        self.model.to(self.device)
        downscale_factor = 2
        width = int(frame.shape[1] / downscale_factor)
        height = int(frame.shape[0] / downscale_factor)
        frame = cv2.resize(frame, (width, height))
        results = self.model(frame)
        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]
        return labels, cord

    def class_to_label(self, x):
        return self.classes[int(x)]

    def plot_boxes(self, results, frame, height, width, confidence=0.3):
        labels, cord = results
        detections = []

        n = len(labels)
        x_shape, y_shape = width, height

        for i in range(n):
            row = cord[i]

            if row[4] >= confidence:
                x1, y1, x2, y2 = int(row[0] * x_shape), int(row[1] * y_shape), int(row[2] * x_shape), int(row[3] * y_shape)

                if self.class_to_label(labels[i]) == "person":
                    detections.append(([x1, y1, int(x2-x1), int(y2-y1)], row[4].item(), "person"))

        return frame, detections

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
detector = YoloDetector(model_name=None)

from deep_sort_realtime.deepsort_tracker import DeepSort

object_tracker = DeepSort(max_age=5, n_init=2, nms_max_overlap=1.0,
                max_cosine_distance=0.3, nn_budget=None,
                embedder="mobilenet", half=True, bgr=True,
                embedder_gpu=True)

while cap.isOpened():
    success, img = cap.read()
    results = detector.score_frame(img)
    img, detections = detector.plot_boxes(results, img, img.shape[0], img.shape[1], confidence=0.5)
    tracks = object_tracker.update_tracks(detections, frame=img)

    for track in tracks:
        if not track.is_confirmed():
            continue
        track_id = track.track_id
        ltrb = track.to_ltrb()
        bbox = ltrb
        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)
        cv2.putText(img, "ID: " + str(track_id), (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    cv2.imshow('img', img)
    if cv2.waitKey(1) == ord("q"):
        break
cap.release()
cv2.destroyAllWindows()</code></pre>

  <h2>Vehicle Detection and Counting</h2>
  <pre><code>import cv2
import numpy as np
import torch

cap = cv2.VideoCapture("Cars.mp4")
model = torch.hub.load("ultralytics/yolov5", "yolov5s", pretrained=True)
cnt = 0

while True:
    ret, frame = cap.read()
    results = model(frame)
    height = frame.shape[0]
    for result in results.xyxy[0]:
        if int(result[3]) >= int(0.6 * height - 2):
            cv2.rectangle(frame, (int(result[0]), int(result[1])), (int(result[2]), int(result[3])), (0, 255, 0), 2)
            cnt += 1
    cv2.line(frame, (0, int(0.6 * height)), (frame.shape[1], int(0.6 * height)), (0, 255, 0), 2)
    cv2.putText(frame, "count:"+str(cnt), (30, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 1, cv2.LINE_AA)
    cv2.imshow("video", frame)
    if cv2.waitKey(1) == ord("q"):
        break
cap.release()
cv2.destroyAllWindows()</code></pre>

  <h2>Video Segmentation with KMeans</h2>
  <pre><code>import numpy as np
from sklearn.cluster import KMeans
import os
import cv2

color_array = np.random.choice(range(256), 256 * 256 * 3).reshape(-1,3)
label_model = KMeans(n_clusters=2)
label_model.fit(color_array)

videoPath = 'Cars.mp4'
cap = cv2.VideoCapture(videoPath)

while cap.isOpened():
    ret, frame = cap.read()
    frame = cv2.resize(frame, (256, 256))
    label_class = label_model.predict(frame.reshape(-1,3)).reshape(256,256).astype(np.float32)
    label_class = cv2.cvtColor(label_class, cv2.COLOR_GRAY2BGR)
    cv2.imshow("img", label_class)
    if cv2.waitKey(30) == ord("q"):
        break
cap.release()
cv2.destroyAllWindows()</code></pre>

</body>
</html>
